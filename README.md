# Creating an automated system that scrapes LinkedIn to analyze the top skills required for desired job positions

## Table of contents
README.md --> Project motivation | Goals | Methodology | Results | Conclussions | Limitations | Future Work | Contributions
linkedin_webScraping.ipynb --> The project itself | Sample code used for the project


## Project motivation
### Introduction
![job_market](https://github.com/XReverte/Webscraping_Linkedin/assets/100844285/85dee40a-12c1-407d-bb74-2204fb732441)
*Image generated by Microsoft Bing (2023). graphic_art [1.0]*

In today's **fast-paced and competitive job market**, breaking into the professional world can often feel like navigating a labyrinth. The abundance of qualified candidates has led recruiters to raise the bar, often seeking individuals who are not just capable but overqualified. Additionally, the advent of Applicant Tracking Systems (ATS) has further complicated the job search process. These systems, designed to streamline hiring, filter applications based on keyword matches, sometimes overlooking a candidate's true potential.

This project emerges as a response to these challenges. It leverages technology to demystify the hiring landscape, providing clarity on **what skills are truly valued**. This clarity not only aids job seekers in honing the right competencies but also in presenting themselves in a manner that aligns with what employers are actively seeking, making the professional world a bit more navigable and accessible.

### Motivation
This project was conceived as a means to **hone my abilities as a Data Scientist**. By tackling the complexities of *Web Scraping*, *Natural Language Processing* and *Data Analysis*, I aimed to push my boundaries and expand my technical skill set. Moreover, showcasing this project on my portfolio **demonstrates my proactive approach to solving real-world problems**, highlighting my capability and creativity to potential employers.

One of the primary motivations behind this project was to **identify the top skills required in the data science domain**. Understanding which skills are in high demand allows me to prioritize my learning efforts, ensuring that I am well-prepared to meet the expectations of the job market. This enhances my ability to position myself as a competitive candidate in the professional landscape.

### Disclaimer
To further enhance my skills, I made this project reproducible so it works for any job description and location specified. However, please be aware:

This project was undertaken for **educational purposes** and as a personal tool to assist in my job search. **I do not endorse or take responsibility for any use of this project by others for their own benefit**. Please be aware that web scraping LinkedIn violates their terms and conditions, and any use of this code for scraping LinkedIn is done at your own risk.


## Goals
The primary objective of this project is to **develop an automated, reproducible, versatile, and fault-tolerant system that scrapes LinkedIn to analyze the top skills required for desired job positions**.
- **Automated**: The system should operate autonomously, requiring the user to only input the job position and location. The system will handle the scraping and data analysis without further user intervention.
- **Reproducible**: The process should be repeatable, ensuring consistent results when the same job position and location are queried.
- **Versatile**: The system should work for any desired job position, not limited to a specific role such as Data Scientist.
- **Fault-Tolerant**: To ensure smooth operation, the system must handle errors gracefully, allowing the automation to continue functioning reliably.

These characteristics—automation, reproducibility, versatility, and fault tolerance—are crucial for a useful data science project, **ensuring efficiency, reliability, and broad applicability**.

The system aims to provide a comprehensive analysis of the top skills, **studying their distribution across various dimensions**:
- **Kind of Job Offer**: By querying a broad term like "Data Scientist," the system can identify different roles (e.g., data scientist, data analyst, project manager, machine learning specialist) and understand how top skills differ among these roles.
- **Location**: Similarly, searching by location (e.g., "Spain") can reveal different groups (e.g., Barcelona, Madrid), illustrating regional differences in required skills.
- **Number of Applicants**: The analysis will consider how the top skills vary depending on the number of applicants for a job offer, providing insights into competitive skill sets.
- **Responsibility Level**: The system will explore how required skills vary based on the level of responsibility (e.g., no experience, internship, full-time), offering a nuanced view of career progression requirements.
- **Company Size**: Understanding how the top skills differ depending on the size of the company (e.g., small startups vs. large corporations) will help tailor skill development to specific company profiles.
- **Sector**: The analysis will also cover different industry sectors (e.g., technology, healthcare, finance), showing sector-specific skill demands.

By achieving these goals, the project aims to assist individuals in their personal career development, equipping them with knowledge of in-demand skills tailored to specific job roles, locations, and industry sectors. This comprehensive analysis not only **contributes valuable insights to the broader community of data science professionals** and recruiters but also aspires to **bridge the gap between job seekers and employers**, fostering a more informed and efficient job market.


## Methodology
### Scraping
The scraping component of this project involves several key steps to ensure accurate and efficient data collection from LinkedIn. The entire process is designed to be **automated, reproducible, versatile, and fault-tolerant**. Here’s an outline of the scraping methodology:

#### Log In to LinkedIn & Enter Search Criteria:
The Chrome WebDriver is initialized to control the browser. The browser navigates to the LinkedIn login page, and the user is prompted to enter their credentials, (to avoid problems, one should use an **alternative Linkedin account**). After logging in, the user specifies the desired job position and location for the study.

#### Scrape Job Offers:
The script identifies the job offers listed on the search results page. Using predefined XPath expressions to locate the necessary elements, it extracts relevant details such as **job title, location, number of applicants, responsibility levels, company size, sector, and skill set**.

#### Error Handling:
The script includes mechanisms to handle errors gracefully, such as retrying failed requests and logging errors for later review. This ensures the scraping process continues smoothly even if some elements are not found.

#### Store Data:
Extracted data is compiled into a pandas DataFrame, then it passes through a preliminary adequation process and is exported in .xlsx format for further analysis.

By following these steps, the system can autonomously scrape LinkedIn for job postings, extracting valuable information. This structured approach ensures that the data collected is reliable and comprehensive, providing a solid foundation for the subsequent analysis phase.

#### Disclaimer:
This project was undertaken for educational purposes and as a personal tool to assist in my job search. I made the project reproducible so it works for any job description and location specified. However, **I do not endorse or take responsibility** for any use of this project by others for their own benefit. Please be aware that web scraping LinkedIn violates their terms and conditions, and any use of this code for scraping LinkedIn is done at your own risk.

### Analysis
The analysis component of this project involves studying the skillset across various dimensions. These dimensions can be grouped into two categories:

The first kind analysis: (“Kind of job offer”, “Location”, “Sector”), need of Natural Language Processing and clusterization techniques, so we try to make the code reproductible for them all by defining functions.

The other kind of analysis is much simpler, because it don’t need for NLP nor clusterization techniques. “Nº of applications”, “responsibility” and “employees” pertain to this group. Although they all need for its particular preprocessing techniques, there is nothing worth mentioning here. So, for brevity, and in order to avoid redundancy, we will focus on explaining the methodology for the complex analysis, specifically using "**Kind of Job Offer**" as an example.

#### NLP:
We first create a function to clean and **tokenize** the text. This function standardizes the data by converting it to lowercase, removing non-alphabetic characters, and eliminating English and Spanish stopwords. Then we build another function that filters the tokenized text by removing words that appear only once, ensuring a more logical and intuitive grouping.

A Term Frequency-Inverse Inverse Document Frequency (**TF-IDF**) matrix is created. This matrix measures the frequency of words in a document (TF) and the rarity of words in the corpus (IDF), highlighting significant words for each specific document, indicating its relative importance.

#### Best K and Hyperparameters:
Using the TF-IDF matrix, we determine the optimal number of clusters (K). The **gap statistic** is computed for a range of K values based on the sum of unique words after tokenization and filtering. The scores for each K are normalized and stored in a dictionary.

Alternative methods, such as the **silhouette** score and **elbow method**, are also explored. Each method builds a dictionary of normalized importance values for each K, allowing us to select the overall best K.

Once the best K is selected, hyperparameters are optimized through multiple iterations of **Grid-Search**, leveraging the least extracted best params for the new search. Each model, both for finding K and hyperparameters, has built with cross validation and multiple iterations, ensuring robust and meaningful conclusions.

#### Visualizations:
With the optimal K and hyperparameters, a k-means model is fitted, and cluster labels are added to the dataset, preparing it for group analysis. A **Word Cloud** is generated to visualize the composition and importance of elements within each cluster.

A new DataFrame is created to count the occurrences of each skill across different clusters. **Scatter-plots** are built to represent how the top 30 skills vary for each cluster. For in-depth analysis, similar plots are created focusing on individual clusters, allowing for a detailed examination of specific groups.

## Results & Discussion
In this chapter, we will focus on the results of the "Kind of Job Offer" section, which we believe are the most insightful. See the full project for understanding how top skills vary depending on location, nº of applicants, responsibility level, company size and sector

### Word Cloud:
The Word Cloud visualization allows us to clearly identify the composition of each cluster. Each cluster represents different job roles, with word combinations unique to each group. The size of the words indicates their importance or frequency, while the colors are used for better distinction.

![WordCloud_jobOffers](https://github.com/XReverte/Webscraping_Linkedin/assets/100844285/1d6e5c98-b18d-44ee-8895-e513d71911a4)
*Word Cloud for each cluster, based on their most comon keywords -Clusterized by kind of job offer-*

We identified several highly cohesive clusters, meaning the words within these clusters are strongly related. The clusters (1 – Data Scientist | 2 – Data Engineer | 3 – Data Analyst | 4 – Machine Learning Engineer | 6 – Data Architect) pertain to this group, almost all of them, making clear it has been a **proper clusterization**.

Taking into account that we intend to maintain an automation and versatility philosophy, it’s obvious **the segmentation won’t be perfect**, just efficient and reliable. This is why there is a cluster (0 -Business Intelligence Developer) with a high heterogeneity. Additionally, the group (5 – Product Manager) is quite cohesive, but its identifier may not fully reflect the group's composition.

### Skills Sccatter:
As for the skills scatter-plot visualization, we see how the importance (counts – “y axis”) of the top 30 skills (“x axis”) across different job offers (clusters – “legend”). The size of the points redundantly represents the skill count, and there is a line representing mean counting of the overall groups.

![skills_jobPosition](https://github.com/XReverte/Webscraping_Linkedin/assets/100844285/f378bdee-bde5-42f4-b344-001286176961)
*Top 30 skills sccatter-plot for each cluster -Clusterized by kind of job offer-*

This analysis provides a better understanding of the data scientist job market, revealing several key insights:
-	We now understand that the most demanded programing language is **Python**, followed by SQL.
-	Although both are important, it seems that **Data Analysis** is desirable than Data Engineer, so knowing how to interpret existing data for inform decision-making is preferable than knowing how to build systems and architecture that store and process the data. Big Data is still present but left behind.
-	Having strong **communication skills** and proficiency in **English** is crucial
-	When it comes to soft skills, it's imperative to have a mindset with **analytical capacity** and **problem-solving** skills.
-	Machine Learning skills are fairly present alongside the spectrum of top skills, with **Natural Language Processing** (NLP) being particularly significant.
-	Knowledge in **Computer Science**, **ETL** (Extract, Transform, Load), **Data Visualization**, **Databases**, and **Statistics** is valuable.
-	Familiarity with **Apache Spark**, **AWS**, and **Microsoft Azure** technologies is useful.

To compare different groups, we can study case-oriented plots. For example, focusing on the "**Data Scientist**" group, we view a similar plot, but with the data filtered to obtain the sorted 20 most common skills for that group.

![skills_DS](https://github.com/XReverte/Webscraping_Linkedin/assets/100844285/316c2b50-919c-4a5f-acd4-1bd95449d9af)
*Top 20 skills for 'Data Scientist position' sccatter-plot for each cluster -Clusterized by kind of job offer-*

To study this chart, we only need to focus on the areas where the **line graph is at a minimum** to see which skills are particularly important for this group. Knowing this, we easily find out that **NLP** and **predictive analytics** gain a lot of importance for a data scientist. **Data visualization**, **Deep Learning** and **statistics** skills are also more critical for data scientists compared to other roles.

## Future Work
Future iterations of this project could benefit from a more exhaustive search of clustering models. For instance, exploring **hierarchical clustering** as an alternative to k-means might provide deeper insights into the relationships between job skills. Hierarchical clustering could uncover nested groupings within the data, offering a more nuanced understanding of skill distributions across different job roles.

Further analysis could be enriched by **categorizing skills** into specific groups such as soft skills, technologies, programming languages, and mindset attributes. By segmenting skills into these categories, we could gain a more detailed view of which types of skills are most critical for various job positions. This categorization would allow for targeted recommendations for job seekers looking to improve in specific areas.

**Additional dimensions** could be incorporated into the analysis to provide a more comprehensive view of the job market. For example, analyzing trends over time could reveal how the demand for certain skills evolves, or incorporating company-specific data could highlight how skill requirements vary across different organizational cultures and business models.

## Contributions
This project represents a significant step towards understanding the dynamic landscape of job market demands, particularly in the field of data science. The contributions of this project are multi-faceted:

An automated system was developed to scrape LinkedIn for job postings, efficiently collecting data on the top skills required for various job positions. This system is: Automated, Reproducible, Versatile and Fault-Tolerant.

By leveraging Natural Language Processing and clustering techniques, this project provides an in-depth analysis of the skills in demand across different job roles and sectors. The project includes detailed visualizations that enhance understanding of the data.

By making this project available on GitHub, it contributes to the open-source community, allowing others to learn from, build upon, and enhance the work. This promotes collaboration and continuous improvement within the field.

The project includes a clear disclaimer regarding the ethical and legal implications of web scraping, emphasizing the importance of adhering to LinkedIn's terms and conditions.

## Tools
